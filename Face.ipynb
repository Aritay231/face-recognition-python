{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8337238-5868-4660-98bc-f98621d73392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv2 import dnn\n",
    "from math import ceil\n",
    "\n",
    "image_mean = np.array([127, 127, 127])\n",
    "image_std = 128.0\n",
    "iou_threshold = 0.3\n",
    "center_variance = 0.1\n",
    "size_variance = 0.2\n",
    "min_boxes = [\n",
    "    [10.0, 16.0, 24.0], \n",
    "    [32.0, 48.0], \n",
    "    [64.0, 96.0], \n",
    "    [128.0, 192.0, 256.0]\n",
    "]\n",
    "strides = [8.0, 16.0, 32.0, 64.0]\n",
    "threshold = 0.5\n",
    "\n",
    "def define_img_size(image_size):\n",
    "    shrinkage_list = []\n",
    "    feature_map_w_h_list = []\n",
    "    for size in image_size:\n",
    "        feature_map = [int(ceil(size / stride)) for stride in strides]\n",
    "        feature_map_w_h_list.append(feature_map)\n",
    "\n",
    "    for i in range(0, len(image_size)):\n",
    "        shrinkage_list.append(strides)\n",
    "    priors = generate_priors(\n",
    "        feature_map_w_h_list, shrinkage_list, image_size, min_boxes\n",
    "    )\n",
    "    return priors\n",
    "\n",
    "\n",
    "def generate_priors(\n",
    "    feature_map_list, shrinkage_list, image_size, min_boxes\n",
    "):\n",
    "    priors = []\n",
    "    for index in range(0, len(feature_map_list[0])):\n",
    "        scale_w = image_size[0] / shrinkage_list[0][index]\n",
    "        scale_h = image_size[1] / shrinkage_list[1][index]\n",
    "        for j in range(0, feature_map_list[1][index]):\n",
    "            for i in range(0, feature_map_list[0][index]):\n",
    "                x_center = (i + 0.5) / scale_w\n",
    "                y_center = (j + 0.5) / scale_h\n",
    "\n",
    "                for min_box in min_boxes[index]:\n",
    "                    w = min_box / image_size[0]\n",
    "                    h = min_box / image_size[1]\n",
    "                    priors.append([\n",
    "                        x_center,\n",
    "                        y_center,\n",
    "                        w,\n",
    "                        h\n",
    "                    ])\n",
    "    return np.clip(priors, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def choose_best_score(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n",
    "    scores = box_scores[:, -1]\n",
    "    boxes = box_scores[:, :-1]\n",
    "    picked = []\n",
    "    indexes = np.argsort(scores)\n",
    "    indexes = indexes[-candidate_size:]\n",
    "    while len(indexes) > 0:\n",
    "        current = indexes[-1]\n",
    "        picked.append(current)\n",
    "        if 0 < top_k == len(picked) or len(indexes) == 1:\n",
    "            break\n",
    "        current_box = boxes[current, :]\n",
    "        indexes = indexes[:-1]\n",
    "        rest_boxes = boxes[indexes, :]\n",
    "        iou = iou_of(\n",
    "            rest_boxes,\n",
    "            np.expand_dims(current_box, axis=0),\n",
    "        )\n",
    "        indexes = indexes[iou <= iou_threshold]\n",
    "    return box_scores[picked, :]\n",
    "\n",
    "\n",
    "def area_of(left_top, right_bottom):\n",
    "    hw = np.clip(right_bottom - left_top, 0.0, None)\n",
    "    return hw[..., 0] * hw[..., 1]\n",
    "\n",
    "\n",
    "def iou_of(boxes0, boxes1, eps=1e-5):\n",
    "    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n",
    "    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:]) \n",
    "\n",
    "    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n",
    "    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n",
    "    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n",
    "    return overlap_area / (area0 + area1 - overlap_area + eps)\n",
    "\n",
    "\n",
    "def predict(\n",
    "    width, \n",
    "    height, \n",
    "    confidences, \n",
    "    boxes, \n",
    "    prob_threshold, \n",
    "    iou_threshold=0.3, \n",
    "    top_k=-1\n",
    "):\n",
    "    boxes = boxes[0]\n",
    "    confidences = confidences[0]\n",
    "    picked_box_probs = []\n",
    "    picked_labels = []\n",
    "    for class_index in range(1, confidences.shape[1]):\n",
    "        probs = confidences[:, class_index]\n",
    "        mask = probs > prob_threshold\n",
    "        probs = probs[mask]\n",
    "        if probs.shape[0] == 0:\n",
    "            continue\n",
    "        subset_boxes = boxes[mask, :]\n",
    "        box_probs = np.concatenate(\n",
    "            [subset_boxes, probs.reshape(-1, 1)], axis=1\n",
    "        )\n",
    "        box_probs = choose_score(box_probs,\n",
    "                             iou_threshold=iou_threshold,\n",
    "                             top_k=top_k,\n",
    "                             )\n",
    "        picked_box_probs.append(box_probs)\n",
    "        picked_labels.extend([class_index] * box_probs.shape[0])\n",
    "    if not picked_box_probs:\n",
    "        return np.array([]), np.array([]), np.array([])\n",
    "    picked_box_probs = np.concatenate(picked_box_probs)\n",
    "    picked_box_probs[:, 0] *= width\n",
    "    picked_box_probs[:, 1] *= height\n",
    "    picked_box_probs[:, 2] *= width\n",
    "    picked_box_probs[:, 3] *= height\n",
    "    return (\n",
    "        picked_box_probs[:, :4].astype(np.int32), \n",
    "        np.array(picked_labels), \n",
    "        picked_box_probs[:, 4]\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_locations_to_boxes(locations, priors, center_variance,\n",
    "                               size_variance):\n",
    "    if len(priors.shape) + 1 == len(locations.shape):\n",
    "        priors = np.expand_dims(priors, 0)\n",
    "    return np.concatenate([\n",
    "        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n",
    "        np.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n",
    "    ], axis=len(locations.shape) - 1)\n",
    "\n",
    "\n",
    "def center_form_to_corner_form(locations):\n",
    "    return np.concatenate(\n",
    "        [locations[..., :2] - locations[..., 2:] / 2,\n",
    "         locations[..., :2] + locations[..., 2:] / 2], \n",
    "        len(locations.shape) - 1\n",
    "    )\n",
    "\n",
    "\n",
    "def FER_live_cam():\n",
    "    emotion_dict = {\n",
    "        0: 'neutral', \n",
    "        1: 'happiness', \n",
    "        2: 'surprise', \n",
    "        3: 'sadness',\n",
    "        4: 'anger', \n",
    "        5: 'disgust', \n",
    "        6: 'fear'\n",
    "    }\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    size = (frame_width, frame_height)\n",
    "\n",
    "    model = 'onnx_model.onnx'\n",
    "    model = cv2.dnn.readNetFromONNX('emotion-ferplus-8.onnx')\n",
    "    \n",
    "    model_path = 'RFB-320/RFB-320.caffemodel'\n",
    "    proto_path = 'RFB-320/RFB-320.prototxt'\n",
    "    net = dnn.readNetFromCaffe(proto_path, model_path)\n",
    "    input_size = [320, 240]\n",
    "    width = input_size[0]\n",
    "    height = input_size[1]\n",
    "    priors = define_img_size(input_size)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame , 1)\n",
    "        if ret:\n",
    "            img_ori = frame\n",
    "            rect = cv2.resize(img_ori, (width, height))\n",
    "            rect = cv2.cvtColor(rect, cv2.COLOR_BGR2RGB)\n",
    "            net.setInput(dnn.blobFromImage(\n",
    "                rect, 1 / image_std, (width, height), 127)\n",
    "            )\n",
    "            boxes, scores = net.forward([\"boxes\", \"scores\"])\n",
    "            boxes = np.expand_dims(np.reshape(boxes, (-1, 4)), axis=0)\n",
    "            scores = np.expand_dims(np.reshape(scores, (-1, 2)), axis=0)\n",
    "            boxes = convert_locations_to_boxes(\n",
    "                boxes, priors, center_variance, size_variance\n",
    "            )\n",
    "            boxes = center_form_to_corner_form(boxes)\n",
    "            boxes, labels, probs = predict(\n",
    "                img_ori.shape[1], \n",
    "                img_ori.shape[0], \n",
    "                scores, \n",
    "                boxes, \n",
    "                threshold\n",
    "            )\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            for (x1, y1, x2, y2) in boxes:\n",
    "                x1 = np.clip(x1, 0, frame.shape[1])\n",
    "                y1 = np.clip(y1, 0, frame.shape[0])\n",
    "                x2 = np.clip(x2, 0, frame.shape[1])\n",
    "                y2 = np.clip(y2, 0, frame.shape[0])\n",
    "                w = x2 - x1\n",
    "                h = y2 - y1\n",
    "                cv2.rectangle(frame, (x1,y1), (x2, y2), (255,0,0), 2)\n",
    "                resize_frame = cv2.resize(\n",
    "                    gray[y1:y1 + h, x1:x1 + w], (64, 64)\n",
    "                )\n",
    "                resize_frame = resize_frame.reshape(1, 1, 64, 64)\n",
    "                model.setInput(resize_frame)\n",
    "                output = model.forward()\n",
    "                pred = emotion_dict[list(output[0]).index(max(output[0]))]\n",
    "                cv2.rectangle(\n",
    "                    img_ori, \n",
    "                    (x1, y1), \n",
    "                    (x2, y2), \n",
    "                    (255, 255, 255), \n",
    "                    2,\n",
    "                    lineType=cv2.LINE_AA\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    pred, \n",
    "                    (x1, y1-10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.8, \n",
    "                    (255, 0, 0), \n",
    "                    2,\n",
    "                    lineType=cv2.LINE_AA\n",
    "                )\n",
    "        \n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FER_live_cam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b7779-eb34-4e83-a868-0d840e317600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
